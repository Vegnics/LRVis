{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b215f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', './', './', '/home/quinoa/.conda/envs/lwvis/lib/python311.zip', '/home/quinoa/.conda/envs/lwvis/lib/python3.11', '/home/quinoa/.conda/envs/lwvis/lib/python3.11/lib-dynload', '', '/home/quinoa/.conda/envs/lwvis/lib/python3.11/site-packages'] /home/quinoa/LRVis/examples\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# train_stl10_256.py\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, transforms\n",
    "import sys,os\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(0,\"../\") \n",
    "print(sys.path,os.getcwd())\n",
    "\n",
    "# ----------------------------\n",
    "# Paste / import your model here\n",
    "# from model import preact_resnet18_bottleneck\n",
    "# ----------------------------\n",
    "from modules.lowrank import preact_resnet18_bottleneck\n",
    "\n",
    "\n",
    "def accuracy_topk(logits, targets, topk=(1, 5)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        out = []\n",
    "        for k in topk:\n",
    "            out.append(correct[:k].reshape(-1).float().sum().mul_(100.0 / targets.size(0)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None, log_every=50):\n",
    "    model.train()\n",
    "    total_loss, total_top1, total_top5, n = 0.0, 0.0, 0.0, 0\n",
    "    for it, (images, targets) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        #print(images)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        top1, top5 = accuracy_topk(logits, targets, topk=(1, 5))\n",
    "\n",
    "        bs = images.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_top1 += top1.item() * bs\n",
    "        total_top5 += top5.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        if (it + 1) % log_every == 0:\n",
    "            print(\n",
    "                f\"iter {it+1:5d}/{len(loader)} | \"\n",
    "                f\"loss {total_loss/n:.4f} | top1 {total_top1/n:.2f}% | top5 {total_top5/n:.2f}%\"\n",
    "            )\n",
    "\n",
    "    return total_loss / n, total_top1 / n, total_top5 / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_top1, total_top5, n = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, targets)\n",
    "        top1, top5 = accuracy_topk(logits, targets, topk=(1, 5))\n",
    "\n",
    "        bs = images.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_top1 += top1.item() * bs\n",
    "        total_top5 += top5.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / n, total_top1 / n, total_top5 / n\n",
    "\n",
    "\n",
    "def save_checkpoint(path, model, optimizer, epoch, best_top1):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_top1\": best_top1,\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, default=\"./data_stl10\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=256)\n",
    "    parser.add_argument(\"--workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--wd\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--label-smoothing\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=\"./checkpoints_stl10_256\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()  # colab-friendly\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # ---- Transforms (force 256x256) ----\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(512, scale=(0.5, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "        #                     std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        #transforms.Resize(288),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "        #                     std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    #\"\"\"\n",
    "    # ---- Caltech-256 ----\n",
    "    \"\"\"full_set = datasets.Caltech256(\n",
    "        root=args.data,\n",
    "        download= True, #True,\n",
    "        transform=train_tf  # temp, we'll override val below\n",
    "    )\n",
    "    \"\"\"\n",
    "    train_set = ImageNetCustom(\n",
    "        root=\"/media/quinoa/Imagenet\",\n",
    "        split=\"train\",\n",
    "        transform=train_tf  # temp, we'll override val below\n",
    "    )\n",
    "    #print(full_set.index,\"\\n\", full_set.y)\n",
    "\n",
    "    #print(full_set)\n",
    "\n",
    "    num_classes = 257\n",
    "    total = len(full_set)\n",
    "\n",
    "    # 90/10 train/val split\n",
    "    #train_size = int(0.4 * total)\n",
    "    #val_size = int(0.2*(total - train_size))\n",
    "    train_size = int(0.8*total)\n",
    "    val_size = total - train_size\n",
    "    #trash_size = total - train_size - val_size\n",
    "\n",
    "    #train_set, val_set,_ = random_split(full_set, [train_size, val_size,trash_size])\n",
    "    train_set, val_set = random_split(full_set, [train_size, val_size])\n",
    "    #print(total,train_size,val_size,trash_size)\n",
    "    # Apply val transform to val subset\n",
    "    val_set.dataset.transform = val_tf\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        #collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        #collate_fn = collate_fn\n",
    "    )\n",
    "    #\"\"\"\n",
    "\n",
    "    # ---- STL10 ----\n",
    "    \"\"\"\n",
    "    train_set = datasets.STL10(root=args.data, split=\"train\", download=True, transform=train_tf)\n",
    "    val_set = datasets.STL10(root=args.data, split=\"test\", download=True, transform=val_tf)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    num_classes = 10\n",
    "\n",
    "\n",
    "    # ---- GTSRB ----\n",
    "    train_set = datasets.GTSRB(\n",
    "        root=args.data,\n",
    "        split=\"train\",\n",
    "        download=True,\n",
    "        transform=train_tf\n",
    "    )\n",
    "\n",
    "    val_set = datasets.GTSRB(\n",
    "        root=args.data,\n",
    "        split=\"test\",\n",
    "        download=True,\n",
    "        transform=val_tf\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    num_classes = 43\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Train samples: {len(train_set)} | Test samples: {len(val_set)} | Classes: {num_classes}\")\n",
    "\n",
    "\n",
    "    # ---- Model ----\n",
    "    model = preact_resnet18_bottleneck(num_classes=num_classes, in_ch=3,useLR=True).to(device)\n",
    "    ## Debugging the model\n",
    "    #print(model)\n",
    "\n",
    "    # ---- Loss / Optim / Scheduler ----\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=[0.9,0.99], weight_decay=args.wd,eps=1e-3)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,100,160,180], gamma=0.1)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if (args.amp and device.type == \"cuda\") else None\n",
    "\n",
    "    # ---- Resume ----\n",
    "    start_epoch = 0\n",
    "    best_top1 = 0.0\n",
    "    if args.resume:\n",
    "        ckpt = torch.load(args.resume, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        start_epoch = ckpt.get(\"epoch\", 0) + 1\n",
    "        best_top1 = ckpt.get(\"best_top1\", 0.0)\n",
    "        print(f\"Resumed from {args.resume} @ epoch {start_epoch}, best_top1={best_top1:.2f}\")\n",
    "\n",
    "    save_dir = Path(args.save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Train ----\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{args.epochs} | lr={optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        tr_loss, tr_top1, tr_top5 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, scaler=scaler, log_every=50\n",
    "        )\n",
    "        va_loss, va_top1, va_top5 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:3d} | \"\n",
    "            f\"train: loss {tr_loss:.4f} top1 {tr_top1:.2f}% top5 {tr_top5:.2f}% | \"\n",
    "            f\"test:  loss {va_loss:.4f} top1 {va_top1:.2f}% top5 {va_top5:.2f}%\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        save_checkpoint(save_dir / \"last.pt\", model, optimizer, epoch, best_top1)\n",
    "        if va_top1 > best_top1:\n",
    "            best_top1 = va_top1\n",
    "            save_checkpoint(save_dir / \"best.pt\", model, optimizer, epoch, best_top1)\n",
    "            print(f\"  saved best.pt (top1={best_top1:.2f}%)\")\n",
    "\n",
    "    print(\"\\nDone. Best top1:\", best_top1)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"train_stl10_256.py\",\n",
    "    \"--data\", \"../curated\",\n",
    "    \"--epochs\", \"200\",\n",
    "    \"--batch-size\", \"92\",\n",
    "    \"--lr\", \"0.0002\",\n",
    "    \"--amp\",\n",
    "    \"--label-smoothing\", \"0.15\",\n",
    "]\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2abc11-dcb2-4ee0-9e58-0f6867ace4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 257/257 [02:41<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "FOLDER = \"../caltech256/256_ObjectCategories\"\n",
    "!mkdir -p ../curated/caltech256\n",
    "OUT_FOLDER = \"../curated/caltech256/256_ObjectCategories\"\n",
    "for a in tqdm(os.listdir(FOLDER)):\n",
    "  os.makedirs(os.path.join(OUT_FOLDER,a),exist_ok=True)\n",
    "  fpaths = os.listdir(os.path.join(FOLDER,a))\n",
    "  for b in glob(os.path.join(FOLDER,a,\"*.[jJ][pP]*[gG]\")):\n",
    "    fname = b.split(\"/\")[-1]\n",
    "    ext = fname.split(\".\")[-1]\n",
    "    if ext != \"jpg\":\n",
    "      print(fname)\n",
    "    img = Image.open(b)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((640,640))\n",
    "    img.save(os.path.join(OUT_FOLDER,a,fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91df052-2d50-47ce-ad7c-3249bcd8d924",
   "metadata": {},
   "source": [
    "## Custom ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7846ef7-21f7-4d4c-a2db-ddb93aaa7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from collections.abc import Iterator\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torchvision.datasets.utils import check_integrity, extract_archive, verify_str_arg\n",
    "\n",
    "ARCHIVE_META = {\n",
    "    \"train\": (\"ILSVRC2010_images_train.tar\", \"1d675b47d978889d74fa0da5fadfb00e\"),\n",
    "    \"val\": (\"ILSVRC2010_images_val.tar\", \"29b22e2961454d5413ddabcf34fc5622\"),\n",
    "    \"devkit\": (\"ILSVRC2010_devkit_t12.tar.gz\", \"fa75699e90414af021442c21a62c3abf\"),\n",
    "}\n",
    "\n",
    "META_FILE = \"meta.bin\"\n",
    "\n",
    "\n",
    "class ImageNetCustom(ImageFolder):\n",
    "    \"\"\"`ImageNet <http://image-net.org/>`_ 2012 Classification Dataset.\n",
    "\n",
    "    .. note::\n",
    "        Before using this class, it is required to download ImageNet 2012 dataset from\n",
    "        `here <https://image-net.org/challenges/LSVRC/2012/2012-downloads.php>`_ and\n",
    "        place the files ``ILSVRC2012_devkit_t12.tar.gz`` and ``ILSVRC2012_img_train.tar``\n",
    "        or ``ILSVRC2012_img_val.tar`` based on ``split`` in the root directory.\n",
    "\n",
    "    Args:\n",
    "        root (str or ``pathlib.Path``): Root directory of the ImageNet Dataset.\n",
    "        split (string, optional): The dataset split, supports ``train``, or ``val``.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image or torch.Tensor, depends on the given loader,\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "            By default, it uses PIL as its image loader, but users could also pass in\n",
    "            ``torchvision.io.decode_image`` for decoding image data into tensors directly.\n",
    "\n",
    "     Attributes:\n",
    "        classes (list): List of the class name tuples.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        wnids (list): List of the WordNet IDs.\n",
    "        wnid_to_idx (dict): Dict with items (wordnet_id, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "        targets (list): The class_index value for each image in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: Union[str, Path], split: str = \"train\", **kwargs: Any) -> None:\n",
    "        root = self.root = os.path.expanduser(root)\n",
    "        self.split = verify_str_arg(split, \"split\", (\"train\", \"val\"))\n",
    "\n",
    "        self.parse_archives()\n",
    "        wnid_to_classes = load_meta_file(self.root)[0]\n",
    "\n",
    "        super().__init__(self.split_folder, **kwargs)\n",
    "        self.root = root\n",
    "\n",
    "        self.wnids = self.classes\n",
    "        self.wnid_to_idx = self.class_to_idx\n",
    "        self.classes = [wnid_to_classes[wnid] for wnid in self.wnids]\n",
    "        self.class_to_idx = {cls: idx for idx, clss in enumerate(self.classes) for cls in clss}\n",
    "\n",
    "    def parse_archives(self) -> None:\n",
    "        if not check_integrity(os.path.join(self.root, META_FILE)):\n",
    "            parse_devkit_archive(self.root)\n",
    "\n",
    "        if not os.path.isdir(self.split_folder):\n",
    "            if self.split == \"train\":\n",
    "                parse_train_archive(self.root)\n",
    "            elif self.split == \"val\":\n",
    "                parse_val_archive(self.root)\n",
    "\n",
    "    @property\n",
    "    def split_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.split)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {split}\".format(**self.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "def load_meta_file(root: Union[str, Path], file: Optional[str] = None) -> tuple[dict[str, str], list[str]]:\n",
    "    if file is None:\n",
    "        file = META_FILE\n",
    "    file = os.path.join(root, file)\n",
    "\n",
    "    if check_integrity(file):\n",
    "        return torch.load(file, weights_only=True)\n",
    "    else:\n",
    "        msg = (\n",
    "            \"The meta file {} is not present in the root directory or is corrupted. \"\n",
    "            \"This file is automatically created by the ImageNet dataset.\"\n",
    "        )\n",
    "        raise RuntimeError(msg.format(file, root))\n",
    "\n",
    "\n",
    "def _verify_archive(root: Union[str, Path], file: str, md5: str) -> None:\n",
    "    #if not check_integrity(os.path.join(root, file), md5):\n",
    "    if not check_integrity(os.path.join(root, file)):\n",
    "        msg = (\n",
    "            \"The archive {} is not present in the root directory or is corrupted. \"\n",
    "            \"You need to download it externally and place it in {}.\"\n",
    "        )\n",
    "        raise RuntimeError(msg.format(file, root))\n",
    "\n",
    "\n",
    "def parse_devkit_archive(root: Union[str, Path], file: Optional[str] = None) -> None:\n",
    "    \"\"\"Parse the devkit archive of the ImageNet2012 classification dataset and save\n",
    "    the meta information in a binary file.\n",
    "\n",
    "    Args:\n",
    "        root (str or ``pathlib.Path``): Root directory containing the devkit archive\n",
    "        file (str, optional): Name of devkit archive. Defaults to\n",
    "            'ILSVRC2012_devkit_t12.tar.gz'\n",
    "    \"\"\"\n",
    "    import scipy.io as sio\n",
    "\n",
    "    def parse_meta_mat(devkit_root: str) -> tuple[dict[int, str], dict[str, tuple[str, ...]]]:\n",
    "        metafile = os.path.join(devkit_root, \"data\", \"meta.mat\")\n",
    "        meta = sio.loadmat(metafile, squeeze_me=True)[\"synsets\"]\n",
    "        nums_children = list(zip(*meta))[4]\n",
    "        meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "        idcs, wnids, classes = list(zip(*meta))[:3]\n",
    "        classes = [tuple(clss.split(\", \")) for clss in classes]\n",
    "        idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n",
    "        wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n",
    "        return idx_to_wnid, wnid_to_classes\n",
    "\n",
    "    def parse_val_groundtruth_txt(devkit_root: str) -> list[int]:\n",
    "        file = os.path.join(devkit_root, \"data\", \"ILSVRC2010_validation_ground_truth.txt\")\n",
    "        with open(file) as txtfh:\n",
    "            val_idcs = txtfh.readlines()\n",
    "        return [int(val_idx) for val_idx in val_idcs]\n",
    "\n",
    "    @contextmanager\n",
    "    def get_tmp_dir() -> Iterator[str]:\n",
    "        tmp_dir = tempfile.mkdtemp()\n",
    "        try:\n",
    "            yield tmp_dir\n",
    "        finally:\n",
    "            shutil.rmtree(tmp_dir)\n",
    "\n",
    "    archive_meta = ARCHIVE_META[\"devkit\"]\n",
    "    if file is None:\n",
    "        file = archive_meta[0]\n",
    "    md5 = archive_meta[1]\n",
    "\n",
    "    _verify_archive(root, file, md5)\n",
    "\n",
    "    with get_tmp_dir() as tmp_dir:\n",
    "        extract_archive(os.path.join(root, file), tmp_dir)\n",
    "\n",
    "        devkit_root = os.path.join(tmp_dir, \"ILSVRC2010_devkit_t12\")\n",
    "        idx_to_wnid, wnid_to_classes = parse_meta_mat(devkit_root)\n",
    "        val_idcs = parse_val_groundtruth_txt(devkit_root)\n",
    "        val_wnids = [idx_to_wnid[idx] for idx in val_idcs]\n",
    "\n",
    "        torch.save((wnid_to_classes, val_wnids), os.path.join(root, META_FILE))\n",
    "\n",
    "\n",
    "def parse_train_archive(root: Union[str, Path], file: Optional[str] = None, folder: str = \"train\") -> None:\n",
    "    \"\"\"Parse the train images archive of the ImageNet2012 classification dataset and\n",
    "    prepare it for usage with the ImageNet dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or ``pathlib.Path``): Root directory containing the train images archive\n",
    "        file (str, optional): Name of train images archive. Defaults to\n",
    "            'ILSVRC2012_img_train.tar'\n",
    "        folder (str, optional): Optional name for train images folder. Defaults to\n",
    "            'train'\n",
    "    \"\"\"\n",
    "    archive_meta = ARCHIVE_META[\"train\"]\n",
    "    if file is None:\n",
    "        file = archive_meta[0]\n",
    "    md5 = archive_meta[1]\n",
    "\n",
    "    _verify_archive(root, file, md5)\n",
    "\n",
    "    train_root = os.path.join(root, folder)\n",
    "    extract_archive(os.path.join(root, file), train_root)\n",
    "\n",
    "    archives = [os.path.join(train_root, archive) for archive in os.listdir(train_root)]\n",
    "    for archive in archives:\n",
    "        extract_archive(archive, os.path.splitext(archive)[0], remove_finished=True)\n",
    "\n",
    "\n",
    "def parse_val_archive(\n",
    "    root: Union[str, Path], file: Optional[str] = None, wnids: Optional[list[str]] = None, folder: str = \"val\"\n",
    ") -> None:\n",
    "    \"\"\"Parse the validation images archive of the ImageNet2012 classification dataset\n",
    "    and prepare it for usage with the ImageNet dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or ``pathlib.Path``): Root directory containing the validation images archive\n",
    "        file (str, optional): Name of validation images archive. Defaults to\n",
    "            'ILSVRC2012_img_val.tar'\n",
    "        wnids (list, optional): List of WordNet IDs of the validation images. If None\n",
    "            is given, the IDs are loaded from the meta file in the root directory\n",
    "        folder (str, optional): Optional name for validation images folder. Defaults to\n",
    "            'val'\n",
    "    \"\"\"\n",
    "    archive_meta = ARCHIVE_META[\"val\"]\n",
    "    if file is None:\n",
    "        file = archive_meta[0]\n",
    "    md5 = archive_meta[1]\n",
    "    if wnids is None:\n",
    "        wnids = load_meta_file(root)[1]\n",
    "\n",
    "    _verify_archive(root, file, md5)\n",
    "\n",
    "    val_root = os.path.join(root, folder)\n",
    "    extract_archive(os.path.join(root, file), val_root)\n",
    "\n",
    "    images = sorted(os.path.join(val_root, image) for image in os.listdir(val_root))\n",
    "\n",
    "    for wnid in set(wnids):\n",
    "        os.mkdir(os.path.join(val_root, wnid))\n",
    "\n",
    "    for wnid, img_file in zip(wnids, images):\n",
    "        shutil.move(img_file, os.path.join(val_root, wnid, os.path.basename(img_file)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b3ddb-5194-4b8f-a2fc-5b7aeb8e666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c8920-c5ea-4bd3-bf54-64c13c112d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lwvis_env",
   "language": "python",
   "name": "lwvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
